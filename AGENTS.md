## Project Overview

mbf_fastq_processor is a Rust-based FASTQ processing tool that filters, samples, slices, demultiplexes, 
and performs various analyses on FASTQ files. 

It uses TOML configuration files to define processing pipelines with multiple transformation steps. 
Hints on the TOML configuration can be found in docs/content/docs/reference/llm-guide.md.

The project emphasizes correctness, flexibility, speed, and reproducible results.

# Repository Guidelines

## Project Structure & Module Organization

The Rust workspace lives under `mbf-fastq-processor/src/`. `src/main.rs` hosts
the CLI entry point and delegates to the library in `mbf-fastq-processorsrc/lib.rs`, while
specialized pipelines sit in modules like `mbf-fastq-processor/src/transformations.rs`,
`mbf-fastq-processor/src/demultiplex.rs`, and the configs under `mbf-fastq-processor/src/config/`.  Integration assets
reside in `test_cases/` (fixture FASTQs, expected outputs) and are orchestrated
by harnesses under `mbf-fastq-processor/tests/`. 

Developer utilities and scripts live in `dev/`,
documentation drafts in `docs/`, and benchmark harnesses in `benchmarks/`.

## Build, Test, and Development Commands

Use Cargo for day-to-day work: `cargo build` for debug builds, `cargo build
--release` for optimized artifacts, and `cargo run -- <config.toml>` to
exercise the CLI locally. `cargo check` gives a fast type check, while `cargo
clippy --all-targets -- -D clippy::pedantic` enforces lint compliance. When
test cases change, run `python3 dev/_update_tests.py` before executing
`cargo test` to regenerate derived tests; add `-- --ignored` to cover
long-running cases. Coverage reports come from `python3 dev/coverage.py --summary`
or `--html`. A reproducible toolchain is available through `nix develop`.
Run cargo through nix if you receive 'unknown command: cargo' errors.

To run an individual test, cd to the test case directory and run `cargo run -- verify`

## Configuration language.

The tests are all in our TOML configuration format.
Read `docs/content/docs/reference/llm-guide.md` for an overview.

## Coding Style & Naming Conventions

Follow rustfmt defaults (run `cargo fmt`!). Prefer 4-space indentation, snake_case
for modules/functions, CamelCase for types, and descriptive test names like
`test_valid_demultiplex_template`. Keep public APIs documented with `///` doc
comments and favor explicit error handling via `anyhow::Result`. Run `clippy -D
clippy: pedantic` before submitting.

Each transformation step goes into it's own separate file.

## Testing Guidelines

### Test Structure

The project uses a two-tier testing approach:

1. **Integration Tests** (`test_cases/`): TOML-based test cases that exercise the full pipeline
2. **Meta-Tests** (`mbf-fastq-processor/tests/`): Rust tests that verify the testing framework and CLI behavior

### Integration Tests in `test_cases/`

Each test case is a directory containing:
- `input.toml`: Configuration file defining the processing pipeline
- Input FASTQ files (often symlinks to `sample_data/`)
- Expected output files (`output_*.fq`, reports, etc.)
- Optional: `expected_error.txt` or `expected_error.regex` for error tests
- Optional: `test.sh`, `prep.sh`, `post.sh` for custom test setup/teardown
- Optional: `check.py` for custom output validation
- SHA256 hashes (`*.fq.uncompressed.sha256`) for reproducible verification

### Test Types

1. **Happy Path Tests**: Run the processor and compare outputs to expected files
2. **Error Tests**: Verify the CLI produces the expected error message (exact match or regex)
3. **Custom Script Tests**: Use `test.sh` or `check.py` for complex validation logic

### Test Generation

Tests are auto-generated by running:
```bash
python3 dev/_update_tests.py
```

This script scans all `input*.toml` files in `test_cases/` and `cookbooks/` and generates Rust test functions in `mbf-fastq-processor/tests/generated.rs`. Run this after adding, removing, or renaming test cases.

### Running Tests

```bash
# Run all tests
cargo test

# Run a specific test case directory
cd /path/to/test_case
cargo run -- verify

# Run long-running tests
cargo test -- --ignored

# Generate coverage reports
python3 dev/coverage.py --summary  # or --html
```

### The `verify` Command

The primary test runner is the CLI's `verify` command, which:
1. Processes the input according to the TOML configuration
2. Compares actual outputs to expected outputs (or runs `check.py` if present)
3. For error tests: Verifies the error matches `expected_error.txt` or `expected_error.regex`
4. Supports `--output-dir` to specify where to write actual outputs (defaults to `actual/`)

### Test Case Directories

- `single_step/`: Tests for individual transformation steps
- `demultiplex/`: Tests for demultiplexing workflows
- `integration/`: Multi-step integration tests
- `input/`: Tests for various input formats and sources
- `output/`: Tests for output handling
- `error_handling/`: Tests for error conditions and validation

### Meta-Tests

Located in `mbf-fastq-processor/tests/`:
- `integration_tests.rs`: CLI command testing (validate, verify, process, cookbook, etc.)
- `verify_all_test_cases_present.rs`: Ensures all test cases are registered in generated tests
- `template_and_documentation_verification.rs`: Validates template generation
- `serde_attribute_validation.rs`: Validates serialization attributes
- `generated.rs`: Auto-generated test functions (DO NOT EDIT MANUALLY)

### Best Practices

- Test output files are stored in `.gitignore`'d `actual/` directories
- Use `sample_data/` for shared input fixtures via symlinks
- Include `output_hash_uncompressed = true` in config to generate SHA256 hashes for verification
- For non-deterministic outputs, use `check.py` instead of exact file comparison
- Error messages should be specific; use regex patterns if messages vary slightly
- Test names derived from directory paths are sanitized to valid Rust identifiers

### Coverage

Use the coverage script to measure test coverage:
```bash
python3 dev/coverage.py --summary  # Summary report
python3 dev/coverage.py --html     # Detailed HTML report
```

Do not bother to clean up 'actual' folders in test cases, they're in .gitignore anyway.

## Commit & Pull Request Guidelines

Write concise, sentence-style commit subjects (e.g., `Verify barcodes are
disjoint`) and keep related changes together. PRs should describe the
motivation, outline validation steps (`cargo test`, coverage runs), and link
issues or research notes. Attach screenshots or sample command outputs when
behavior changes, and request review when clippy and tests are clean. Use
jujutsu (jj) for version control, following its branching and merging
conventions.

## AI Planning Notes

Store agent plans in `dev/ai_plans/<name-of-agent>/<next-number>-short-description.md` alongside the corresponding numbered history.

